{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdoyl2jL3l_C",
        "outputId": "f33371f0-d164-47c4-cb26-f91e364f7266"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text: apple is a good company\n",
            "Processed Text: apple good company\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = input(\"Enter the text: \")\n",
        "text = text.lower()\n",
        "words = [token.text for token in nlp(text) if token.text not in stop_words]\n",
        "\n",
        "print(\"Processed Text:\", \" \".join(words))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_frequency(text):\n",
        "    text = ''.join(char.lower() if char.isalnum() or char.isspace() else ' ' for char in text)\n",
        "    words = text.split()\n",
        "    frequency = {}\n",
        "    for word in words:\n",
        "        frequency[word] = frequency.get(word, 0) + 1\n",
        "    return frequency\n",
        "\n",
        "text = input(\"Enter the text: \")\n",
        "\n",
        "frequencies = word_frequency(text)\n",
        "\n",
        "print(\"\\nWord Frequencies:\")\n",
        "for word, count in sorted(frequencies.items()):\n",
        "    print(f\"{word}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-M7grD630Co",
        "outputId": "34e7342c-57c7-4c8a-ca32-5c5489523656"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text: apple is a good company\n",
            "\n",
            "Word Frequencies:\n",
            "a: 1\n",
            "apple: 1\n",
            "company: 1\n",
            "good: 1\n",
            "is: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dqC3PHyD4Unn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS, stem_text\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = [word for word in simple_preprocess(text) if word not in STOPWORDS]\n",
        "    stemmed = [stem_text(token) for token in tokens]\n",
        "    lemmatized = [token.lemma_ for token in nlp(\" \".join(stemmed))]\n",
        "    return lemmatized\n",
        "\n",
        "file_path = \"sample.txt\"\n",
        "with open(file_path, \"r\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "processed_text = preprocess_text(text)\n",
        "print(\"Processed Text:\", processed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOp6xQWD3nwB",
        "outputId": "bbb37d0b-ba86-4e80-892c-27356bb92d55"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Text: ['natur', 'languag', 'process', 'fascin', 'field', 'studi', 'involv', 'token', 'text', 'remov', 'stopword', 'perform', 'task', 'like', 'stem', 'lemmat', 'enabl', 'machin', 'understand', 'human', 'languag', 'well']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Natural Language Processing is a fascinating field of study. It involves tokenizing text, removing stopwords, and performing tasks like stemming and lemmatization. This enables machines to understand human language better.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "words = [token.text for token in doc]\n",
        "\n",
        "print(\"Sentences:\", sentences)\n",
        "print(\"Words:\", words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG83Sftt4ENM",
        "outputId": "356ba34f-2b68-4f83-d9f2-8ab8a28c00bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Natural Language Processing is a fascinating field of study.', 'It involves tokenizing text, removing stopwords, and performing tasks like stemming and lemmatization.', 'This enables machines to understand human language better.']\n",
            "Words: ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'study', '.', 'It', 'involves', 'tokenizing', 'text', ',', 'removing', 'stopwords', ',', 'and', 'performing', 'tasks', 'like', 'stemming', 'and', 'lemmatization', '.', 'This', 'enables', 'machines', 'to', 'understand', 'human', 'language', 'better', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return cleaned_text.lower()\n",
        "\n",
        "input_text = 'Hello, World! Welcome to NLP 101.'\n",
        "cleaned_text = clean_text(input_text)\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAhwavFh5Mkt",
        "outputId": "1000beb0-a88f-4e7c-cb62-991eea59b7c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world welcome to nlp 101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_emails(text):\n",
        "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "    emails = re.findall(email_pattern, text)\n",
        "    return emails\n",
        "\n",
        "input_text = 'Contact us at support@example.com and sales@example.org.'\n",
        "emails = extract_emails(input_text)\n",
        "print(emails)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMBRw6_l5Y-w",
        "outputId": "671b3e62-5d89-4ed2-b457-2312d8a9ee55"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['support@example.com', 'sales@example.org']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_title(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    title = soup.title.string\n",
        "    return title\n",
        "\n",
        "url = 'https://example.com'\n",
        "title = fetch_title(url)\n",
        "print(\"Page Title:\", title)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWNdsbKE5ngC",
        "outputId": "0a4435f2-fe06-443e-97d0-aeacbee7b93b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page Title: Example Domain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hfm_l0Nw52Bn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kdZc6VUc6DWM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}